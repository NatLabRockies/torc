# coding: utf-8

"""
    torc

    Defines the API for the torc service.

    The version of the OpenAPI document: v0.7.0
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictInt, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from torc.openapi_client.models.jobs_sort_method import JobsSortMethod
from typing import Optional, Set
from typing_extensions import Self

class WorkflowModel(BaseModel):
    """
    WorkflowModel
    """ # noqa: E501
    id: Optional[StrictInt] = None
    name: StrictStr = Field(description="Name of the workflow")
    user: StrictStr = Field(description="User that created the workflow")
    description: Optional[StrictStr] = Field(default=None, description="Description of the workflow")
    timestamp: Optional[StrictStr] = Field(default=None, description="Timestamp of workflow creation")
    project: Optional[StrictStr] = Field(default=None, description="Project name or identifier for grouping workflows")
    metadata: Optional[StrictStr] = Field(default=None, description="Arbitrary metadata as JSON string")
    compute_node_expiration_buffer_seconds: Optional[StrictInt] = Field(default=180, description="Inform all compute nodes to shut down this number of seconds before the expiration time. This allows torc to send SIGTERM to all job processes and set all statuses to terminated. Increase the time in cases where the job processes handle SIGTERM and need more time to gracefully shut down. Set the value to 0 to maximize the time given to jobs. If not set, take the database's default value of 90 seconds.")
    compute_node_wait_for_new_jobs_seconds: Optional[StrictInt] = Field(default=90, description="Inform all compute nodes to wait for new jobs for this time period before exiting. Does not apply if the workflow is complete. Default must be >= completion_check_interval_secs + job_completion_poll_interval to avoid exiting before dependent jobs are unblocked.")
    compute_node_ignore_workflow_completion: Optional[StrictBool] = Field(default=False, description="Inform all compute nodes to ignore workflow completions and hold onto allocations indefinitely. Useful for debugging failed jobs and possibly dynamic workflows where jobs get added after starting.")
    compute_node_wait_for_healthy_database_minutes: Optional[StrictInt] = Field(default=20, description="Inform all compute nodes to wait this number of minutes if the database becomes unresponsive.")
    compute_node_min_time_for_new_jobs_seconds: Optional[StrictInt] = Field(default=300, description="Minimum remaining walltime (in seconds) required before a compute node will request new jobs. If the remaining time is less than this value, the compute node will stop requesting new jobs and wait for running jobs to complete. This prevents starting jobs that won't have enough time to complete. Default is 300 seconds (5 minutes).")
    jobs_sort_method: Optional[JobsSortMethod] = JobsSortMethod.GPUS_RUNTIME_MEMORY
    resource_monitor_config: Optional[StrictStr] = Field(default=None, description="Resource monitoring configuration as JSON string")
    slurm_defaults: Optional[StrictStr] = Field(default=None, description="Default Slurm parameters to apply to all schedulers as JSON string")
    use_pending_failed: Optional[StrictBool] = Field(default=False, description="Use PendingFailed status for failed jobs (enables AI-assisted recovery)")
    status_id: Optional[StrictInt] = None
    __properties: ClassVar[List[str]] = ["id", "name", "user", "description", "timestamp", "project", "metadata", "compute_node_expiration_buffer_seconds", "compute_node_wait_for_new_jobs_seconds", "compute_node_ignore_workflow_completion", "compute_node_wait_for_healthy_database_minutes", "compute_node_min_time_for_new_jobs_seconds", "jobs_sort_method", "resource_monitor_config", "slurm_defaults", "use_pending_failed", "status_id"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of WorkflowModel from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of WorkflowModel from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "id": obj.get("id"),
            "name": obj.get("name"),
            "user": obj.get("user"),
            "description": obj.get("description"),
            "timestamp": obj.get("timestamp"),
            "project": obj.get("project"),
            "metadata": obj.get("metadata"),
            "compute_node_expiration_buffer_seconds": obj.get("compute_node_expiration_buffer_seconds") if obj.get("compute_node_expiration_buffer_seconds") is not None else 180,
            "compute_node_wait_for_new_jobs_seconds": obj.get("compute_node_wait_for_new_jobs_seconds") if obj.get("compute_node_wait_for_new_jobs_seconds") is not None else 90,
            "compute_node_ignore_workflow_completion": obj.get("compute_node_ignore_workflow_completion") if obj.get("compute_node_ignore_workflow_completion") is not None else False,
            "compute_node_wait_for_healthy_database_minutes": obj.get("compute_node_wait_for_healthy_database_minutes") if obj.get("compute_node_wait_for_healthy_database_minutes") is not None else 20,
            "compute_node_min_time_for_new_jobs_seconds": obj.get("compute_node_min_time_for_new_jobs_seconds") if obj.get("compute_node_min_time_for_new_jobs_seconds") is not None else 300,
            "jobs_sort_method": obj.get("jobs_sort_method") if obj.get("jobs_sort_method") is not None else JobsSortMethod.GPUS_RUNTIME_MEMORY,
            "resource_monitor_config": obj.get("resource_monitor_config"),
            "slurm_defaults": obj.get("slurm_defaults"),
            "use_pending_failed": obj.get("use_pending_failed") if obj.get("use_pending_failed") is not None else False,
            "status_id": obj.get("status_id")
        })
        return _obj


